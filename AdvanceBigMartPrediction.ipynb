{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9d99a-5070-46a0-9161-ae5bcf64994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "771fa8fa-1ca7-4b88-87d9-38527428bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535d1480-d016-443b-819f-5cfd8c2f0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "# 1. Load the datasets\n",
    "# ---------------------\n",
    "train = pd.read_csv('/Users/manas/Desktop/train_v9rqX0R.csv')\n",
    "test = pd.read_csv('/Users/manas/Desktop/test_AbJTz2l.csv')\n",
    "\n",
    "# Preserve original identifiers for submission\n",
    "test_ids = test[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2009fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 2. Data Cleaning & Missing Values\n",
    "# ----------------------------\n",
    "train['Item_Weight'].fillna(train['Item_Weight'].median(), inplace=True)\n",
    "test['Item_Weight'].fillna(test['Item_Weight'].median(), inplace=True)\n",
    "\n",
    "train['Outlet_Size'].fillna('Unknown', inplace=True)\n",
    "test['Outlet_Size'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Standardize Item_Fat_Content labels\n",
    "train['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'}, inplace=True)\n",
    "test['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'}, inplace=True)\n",
    "\n",
    "# Correct zero Item_Visibility values using median per Item_Identifier\n",
    "visibility_median = train.groupby('Item_Identifier')['Item_Visibility'].median()\n",
    "train.loc[train['Item_Visibility'] == 0, 'Item_Visibility'] = train['Item_Identifier'].map(visibility_median)\n",
    "test.loc[test['Item_Visibility'] == 0, 'Item_Visibility'] = test['Item_Identifier'].map(visibility_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98a8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 3. Feature Engineering\n",
    "# ----------------------------\n",
    "# Create Outlet_Age and drop the original establishment year\n",
    "train['Outlet_Age'] = 2023 - train['Outlet_Establishment_Year']\n",
    "test['Outlet_Age'] = 2023 - test['Outlet_Establishment_Year']\n",
    "train.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "test.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "\n",
    "# Interaction Feature: Multiply Item_Visibility by Item_MRP\n",
    "train['Visibility_MRP_Interaction'] = train['Item_Visibility'] * train['Item_MRP']\n",
    "test['Visibility_MRP_Interaction'] = test['Item_Visibility'] * test['Item_MRP']\n",
    "\n",
    "# Log-transform Item_Visibility to reduce skewness\n",
    "train['Item_Visibility_Log'] = np.log1p(train['Item_Visibility'])\n",
    "test['Item_Visibility_Log'] = np.log1p(test['Item_Visibility'])\n",
    "\n",
    "# Log-transform the target variable for stability\n",
    "train['Log_Sales'] = np.log1p(train['Item_Outlet_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b9c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 4. Prepare Modeling Data\n",
    "# ----------------------------\n",
    "# Drop identifier columns and also drop the target column (\"Item_Outlet_Sales\" and \"Log_Sales\")\n",
    "cols_to_drop = ['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales', 'Log_Sales']\n",
    "train_model = train.drop(columns=cols_to_drop)\n",
    "test_model = test.drop(columns=['Item_Identifier', 'Outlet_Identifier'])\n",
    "\n",
    "# Identify categorical columns for one-hot encoding.\n",
    "categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "\n",
    "# One-hot encode categorical features for both training and test sets.\n",
    "train_model = pd.get_dummies(train_model, columns=categorical_cols, drop_first=True)\n",
    "test_model = pd.get_dummies(test_model, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align the train and test sets to ensure they have the same features.\n",
    "# (Since we dropped \"Log_Sales\" from train_model, it won't be added to test_model.)\n",
    "train_model, test_model = train_model.align(test_model, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Define features and target.\n",
    "X = train_model.copy()  # All features after encoding and alignment.\n",
    "y = train['Log_Sales']   # Use the log-transformed target from the original train DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb9fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 5. Scale Numerical Features\n",
    "# ----------------------------\n",
    "# Identify numerical columns (including engineered features)\n",
    "num_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age', \n",
    "            'Visibility_MRP_Interaction', 'Item_Visibility_Log']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "test_model[num_cols] = scaler.transform(test_model[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 6. Advanced Modeling: Stacking Ensemble\n",
    "# ----------------------------\n",
    "# Define base estimators with tuned hyperparameters\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=300, max_depth=12, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, \n",
    "                             random_state=42, objective='reg:squarederror')),\n",
    "    ('lgb', lgb.LGBMRegressor(n_estimators=300, max_depth=12, learning_rate=0.05, random_state=42))\n",
    "]\n",
    "\n",
    "# Use Ridge as the final estimator in the stacking ensemble.\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the stacking ensemble on the full training data.\n",
    "stack_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 7. Test Predictions & Submission\n",
    "# ----------------------------\n",
    "# Predict log-sales on the test set and then inverse the log transformation.\n",
    "test_preds_log = stack_model.predict(test_model)\n",
    "test_preds = np.expm1(test_preds_log)\n",
    "\n",
    "# Create the submission file using the original identifiers.\n",
    "submission = test_ids.copy()\n",
    "submission['Item_Outlet_Sales'] = test_preds\n",
    "\n",
    "# Save submission file in the current working directory.\n",
    "submission_file_path = os.path.join(os.getcwd(), \"Advanced_BigMart_Sales_Predictions.csv\")\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "print(f\"Submission file saved at: {submission_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b016e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a85809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ee33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927fdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514e1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
